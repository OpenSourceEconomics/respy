{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Method of Moments\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The simulated method of moments (SMM) is some kind of a sledgehammer approach to estimate model parameters. It is a deviation from the general method of moments (GMM) which does not require that the function mapping parameters to moments has a closed-form solution. Thus, we want to minimize the distance between the actual moments and the simulated moments implied by the model parameters. Mathematically, we want to minimize\n",
    "\n",
    "$$\n",
    "    \\min_\\theta \\; (m(\\theta) - m^*)' W (m(\\theta) - m^*)\n",
    "$$\n",
    "\n",
    "where $\\theta$ is the parameter vector, $m(\\theta)$ are the simulated moments implied by the parameters, $m^*$ are the moments from the data and $W$ is the weighting matrix.\n",
    "\n",
    "## The weighting matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import respy as rp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from estimagic.optimization.optimize import minimize\n",
    "from respy.pre_processing.model_processing import process_params_and_options\n",
    "\n",
    "\n",
    "CHOICES = [\"blue_collar\", \"military\", \"white_collar\", \"school\", \"home\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "After learning about this new technique, we want to employ it. Usually, you have a dataset at hand on which to estimate the model parameters. Here, we try something different.\n",
    "\n",
    "As ``respy`` is designed to unify research on discrete choice dynamic programming models, human capital accumulation and occupational choice, we try to replicate older models. These exercises help to learn about new model features and serve as a reference point for the outcomes. The problem is that we usually do not have access to the data of the paper and cannot estimate parameters via maximum likelihood estimation. Luckily, papers report more and less detailed summary statistics of the data. These statistics can be used to estimate parameters by simulated method of moments.\n",
    "\n",
    "In the following, we will estimate the model parameters of Keane and Wolpin (2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, options = rp.get_example_model(\"kw_2000\", with_data=False)\n",
    "options[\"n_periods\"] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collect the actual moments\n",
    "\n",
    "The first step is to collect the moments from the actual data. There are various tables in the paper which present usable statistics. The function below combines all the statistics into a single moments vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moments_from_actual_data():\n",
    "    # Add choice probabilties. First whites, then blacks.\n",
    "    choice_probs_whites = pd.read_csv(\n",
    "        rp.config.TEST_RESOURCES_DIR / \"kw_2000_table_1_whites_choice_probabilities.csv\", index_col=\"age\"\n",
    "    ).reindex(columns=CHOICES).to_numpy().ravel()\n",
    "    choice_probs_blacks = pd.read_csv(\n",
    "        rp.config.TEST_RESOURCES_DIR / \"kw_2000_table_2_blacks_choice_probabilities.csv\", index_col=\"age\"\n",
    "    ).reindex(columns=CHOICES).to_numpy().ravel()\n",
    "    \n",
    "    # Add mean wages.\n",
    "    wages_whites = pd.read_csv(\n",
    "        rp.config.TEST_RESOURCES_DIR / \"kw_2000_table_3_wage_fit_whites.csv\", index_col=\"age\"\n",
    "    ).reindex(columns=CHOICES[:3]).to_numpy().ravel()\n",
    "    wages_blacks = pd.read_csv(\n",
    "        rp.config.TEST_RESOURCES_DIR / \"kw_2000_table_3_wage_fit_blacks.csv\", index_col=\"age\"\n",
    "    ).reindex(columns=CHOICES[:3]).to_numpy().ravel()\n",
    "    \n",
    "    education = pd.read_csv(\n",
    "        rp.config.TEST_RESOURCES_DIR / \"kw_2000_table_5_school_attainment.csv\", index_col=0\n",
    "    ).drop(columns=\"age\").to_numpy().ravel()\n",
    "    \n",
    "    moments = np.hstack((\n",
    "        choice_probs_whites,\n",
    "        choice_probs_blacks,\n",
    "        wages_whites,\n",
    "        wages_blacks,\n",
    "        education,\n",
    "    ))\n",
    "\n",
    "    return moments    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments = calculate_moments_from_actual_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define function to calculate moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate = rp.get_simulate_func(params, options)\n",
    "df = simulate(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moments_from_simulated_data(df):\n",
    "    df = df.query(\"Period <= 10\")\n",
    "    \n",
    "    choice_probs = df.groupby([\"Black\", \"Period\"]).Choice.value_counts(\n",
    "        normalize=True).unstack().reindex(columns=CHOICES).to_numpy().ravel()\n",
    "    \n",
    "    wages = (\n",
    "        df.query(\"1 <= Period <= 10\")\n",
    "        .groupby([\"Black\", \"Period\", \"Choice\"])\n",
    "        .Wage.mean()\n",
    "        .unstack()\n",
    "        .reindex(columns=CHOICES[:3])\n",
    "        .to_numpy()\n",
    "        .ravel()\n",
    "    )\n",
    "\n",
    "    sub = df.query(\"Period == 7\").copy()\n",
    "    sub[\"education_levels\"] = pd.cut(sub.Experience_School, bins=[0, 11, 12, 15, 20])\n",
    "    shares = sub.groupby(\"Black\").education_levels.value_counts(normalize=True).unstack().T\n",
    "    education = shares.append(sub.groupby(\"Black\").Experience_School.mean()).to_numpy().ravel()\n",
    "    \n",
    "    moments = np.hstack((\n",
    "        choice_probs,\n",
    "        wages,\n",
    "        education\n",
    "    ))\n",
    "    \n",
    "    return moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments_ = calc_moments(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choose a weighting matrix.\n",
    "\n",
    "#### 2.1. Bootstrapping the variances of moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_weighting_matrix(df, calc_moments, n_bootstraps, n_individuals):\n",
    "    identifiers = df.Identifier.unique()\n",
    "    \n",
    "    moments = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        idents = np.random.choice(identifiers, size=n_individuals, replace=True)\n",
    "        sample = df.loc[df.Identifier.isin(idents)]\n",
    "        bootstrapped_moments = calc_moments(sample)\n",
    "        moments.append(bootstrapped_moments)\n",
    "    moments = np.column_stack(moments)\n",
    "    \n",
    "    standard_deviations = moments.std(axis=1)\n",
    "    standard_deviations = np.where(\n",
    "        standard_deviations == 0, 10, standard_deviations\n",
    "    )\n",
    "    \n",
    "    weighting_matrix = np.diag(standard_deviations)\n",
    "        \n",
    "    return weighting_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Two-step estimation\n",
    "\n",
    "1. Start with the identity matrix and estimate optimal parameters.\n",
    "2. Use the optimal parameters to calculate the moment errors and use them to compute another weighting matrix.\n",
    "3. Rerun another estimation with the new weighting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smm = rp.get_smm_func(params, options, moments, calculate_moments_from_simulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"group\"] = params.index.get_level_values('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constr = rp.get_parameter_constraints(\"kw_2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, params = minimize(\n",
    "    smm,\n",
    "    params,\n",
    "    \"scipy_L-BFGS-B\",\n",
    "    algo_options={\"maxfun\": 1000},\n",
    "    constraints=constr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.to_csv(\"kw_2000_W.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_weighting_matrix(params, smm):\n",
    "    simulate = smm.keywords[\"simulate\"]\n",
    "    calc_moments = smm.keywords[\"calc_moments\"]\n",
    "    \n",
    "    df = simulate(params)\n",
    "    estimated_moments = calc_moments(df)\n",
    "    moments_error = estimated_moments - moments\n",
    "    weighting_matrix = np.outer(moments_error, moments_error)\n",
    "    \n",
    "    return weighting_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting_matrix = recover_weighting_matrix(params, smm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smm = rp.get_smm_func(params, options, moments, calculate_moments_from_simulated_data, weighting_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"group\"] = params.index.get_level_values('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, params = minimize(\n",
    "    smm,\n",
    "    params,\n",
    "    \"scipy_L-BFGS-B\",\n",
    "    constraints=constr,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
